# ===============================================
# LLM Configuration (LiteLLM Native)
# ===============================================
# LiteLLM automatically detects provider from model name
# and uses standard environment variables for API keys.
# No need to specify LLM_PROVIDER - LiteLLM handles it!

# Model Configuration
LLM_MODEL=openrouter/google/gemini-2.5-flash
# Model to use for queries
# LiteLLM auto-detects provider from model name:
#   OpenRouter: openrouter/google/gemini-2.5-flash
#   OpenAI: gpt-4o-mini, gpt-4o, gpt-3.5-turbo
#   Azure OpenAI: azure/gpt-4, azure/gpt-35-turbo, azure/gpt-4o
#   Anthropic: claude-3-5-sonnet-20241022, claude-3-5-haiku-20241022
#   Google: gemini-2.0-flash-exp, gemini-pro

# Model Parameters
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2048

# ===============================================
# API Keys (LiteLLM Standard Environment Variables)
# ===============================================
# LiteLLM automatically uses these based on model name:
# - OPENAI_API_KEY for OpenAI models (gpt-*)
# - AZURE_API_KEY for Azure OpenAI models (azure/*)
# - ANTHROPIC_API_KEY for Anthropic models (claude-*)
# - OPENROUTER_API_KEY for OpenRouter models (openrouter/*)
# - GOOGLE_API_KEY for Google models (gemini-*)
# Set the API key(s) for the provider(s) you want to use

OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
AZURE_API_KEY=your_azure_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here
GOOGLE_API_KEY=your_google_api_key_here

# OpenRouter Base URL (optional, uses default if not set)
OPENROUTER_API_BASE=https://openrouter.ai/api/v1

# Azure OpenAI Configuration (required for azure/* models)
AZURE_API_BASE=https://your-resource-name.openai.azure.com
# Alternative: AZURE_OPENAI_API_BASE
AZURE_API_VERSION=2024-02-15-preview
# Optional: defaults to latest if not set

# Note: Azure OpenAI content filtering is configured at the Azure resource level
# in the Azure Portal, not via environment variables. If you experience content
# filtering errors:
# 1. Check content filter settings in Azure Portal for your OpenAI resource
# 2. Consider using a different deployment/model with less strict filtering
# 3. Review prompts that might trigger content filters

# ===============================================
# Example Provider Configurations
# ===============================================
# Uncomment and configure ONE of the following examples based on your provider choice

# --- Example 1: OpenRouter (Recommended for flexibility) ---
# LLM_MODEL=openrouter/google/gemini-2.5-flash
# OPENROUTER_API_KEY=sk-or-v1-...
# OPENROUTER_API_BASE=https://openrouter.ai/api/v1
#
# Other OpenRouter model examples:
# LLM_MODEL=openrouter/anthropic/claude-3.5-sonnet
# LLM_MODEL=openrouter/google/gemini-2.0-flash-exp
# LLM_MODEL=openrouter/meta-llama/llama-3.1-70b-instruct

# --- Example 2: OpenAI Direct ---
# LLM_MODEL=gpt-4o-mini
# OPENAI_API_KEY=sk-...
#
# Other OpenAI model examples:
# LLM_MODEL=gpt-4o
# LLM_MODEL=gpt-4-turbo
# LLM_MODEL=gpt-3.5-turbo

# --- Example 3: Azure OpenAI ---
# LLM_MODEL=azure/gpt-4
# AZURE_API_KEY=your_azure_api_key_here
# AZURE_API_BASE=https://your-resource-name.openai.azure.com
# AZURE_API_VERSION=2024-02-15-preview
#
# Other Azure OpenAI model examples:
# LLM_MODEL=azure/gpt-35-turbo
# LLM_MODEL=azure/gpt-4o
# LLM_MODEL=azure/gpt-4-turbo

# --- Example 4: Anthropic (Claude) ---
# LLM_MODEL=claude-3-5-sonnet-20241022
# ANTHROPIC_API_KEY=sk-ant-...
#
# Other Anthropic model examples:
# LLM_MODEL=claude-3-5-haiku-20241022
# LLM_MODEL=claude-3-opus-20240229
# LLM_MODEL=claude-3-sonnet-20240229

# --- Example 5: Google (Gemini) ---
# LLM_MODEL=gemini-2.0-flash-exp
# GOOGLE_API_KEY=your_google_api_key_here
#
# Other Google model examples:
# LLM_MODEL=gemini-pro
# LLM_MODEL=gemini-1.5-pro
# LLM_MODEL=gemini-1.5-flash

# ===============================================
# Observability & Tracing (Optional)
# ===============================================
PHOENIX_API_KEY=your_phoenix_api_key_here
PHOENIX_PROJECT_NAME=cruise-booking-agent
PHOENIX_COLLECTOR_ENDPOINT=https://app.phoenix.arize.com

# ===============================================
# Logging Configuration
# ===============================================
LOG_LEVEL=INFO
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL

LOG_FORMAT=json
# Options: json, text

LOG_FILE=logs/agent-evals.log

# ===============================================
# LiteLLM Debug Logging
# ===============================================
# Enable LiteLLM debug logging to see request/response details
# Options: DEBUG, INFO (or leave empty to disable)
LITELLM_LOG=INFO
# Set to DEBUG for more verbose logging (includes API keys - use with caution)

# Enable detailed debug mode (logs API keys - NOT for production)
LITELLM_DETAILED_DEBUG=false
# Options: true, false
# Warning: This logs API keys and should not be used in production

# Enable JSON formatted logs from LiteLLM
LITELLM_JSON_LOGS=false
# Options: true, false

# ===============================================
# Evaluation Settings
# ===============================================
EVALUATION_MODE=test
# Options: test, production
