{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cruise Booking AI Agent Evaluation\n",
    "#### @author Karthik Kalahasthi https://www.linkedin.com/in/karthikkalahasthi/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get project root (notebook is in evals/notebooks/, so go up 2 levels)\n",
    "current_dir = Path(os.getcwd())\n",
    "if current_dir.name == 'notebooks':\n",
    "    project_root = current_dir.parent.parent\n",
    "else:\n",
    "    project_root = current_dir.parent.parent\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.client import Client\n",
    "from phoenix.evals import OpenAIModel, llm_classify\n",
    "from phoenix.experiments import run_experiment,evaluate_experiment\n",
    "from phoenix.experiments.evaluators import create_evaluator\n",
    "from phoenix.experiments.types import Example\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), '..'))\n",
    "sys.path.insert(0, project_root)\n",
    "from agents.cruise_booking.agent import root_agent\n",
    "from evals.eval_prompts import load_all_prompts\n",
    "all_prompts = load_all_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load/Export dataset from phoenix-arize. Make sure to upload dataset before exporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_client = Client()\n",
    "dataset_identifier = \"golden_dataset_small\"\n",
    "dataset = px_client.datasets.get_dataset(dataset=dataset_identifier)\n",
    "print(f\"\u2705 Retrieved dataset: {dataset_identifier}\")\n",
    "print(dataset[0])\n",
    "print(dataset.examples[0]['input']['Query'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import duckdb after installation\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Re-import duckdb module\n",
    "try:\n",
    "    import duckdb\n",
    "    # Update the duckdb reference in the data_search module\n",
    "    from src.tools import data_search\n",
    "    data_search.duckdb = duckdb\n",
    "    print(f\"\u2705 DuckDB reloaded successfully! Version: {duckdb.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u274c Failed to import DuckDB: {e}\")\n",
    "    print(\"Please install DuckDB: pip install duckdb==1.4.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Cruise data into vector store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure cruise data is loaded\n",
    "from src.tools.data_search import DataSearch\n",
    "from agents.cruise_booking.tools import data_search_tools\n",
    "import os\n",
    "\n",
    "# Get the correct data directory path (relative to project root)\n",
    "data_dir = os.path.join(project_root, 'data')\n",
    "print(f\"Loading data from: {data_dir}\")\n",
    "\n",
    "# Reload data search with correct path\n",
    "data_search = DataSearch(data_dir=data_dir)\n",
    "stats = data_search.get_stats()\n",
    "print(f\"\u2705 Data loaded: {stats['total_cruises']} cruises, {stats['total_pricing_rows']} pricing rows\")\n",
    "\n",
    "# Update the module-level data search instance\n",
    "data_search_tools._data_search = data_search\n",
    "print(\"\u2705 Data search tools reloaded with data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions to call ADK Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.genai import types\n",
    "\n",
    "agent_outputs = []\n",
    "\n",
    "# Create session service and runner (outside the loop)\n",
    "session_service = InMemorySessionService()\n",
    "runner = Runner(\n",
    "    app_name=\"cruise_booking_eval\",\n",
    "    agent=root_agent,\n",
    "    session_service=session_service\n",
    ")\n",
    "\n",
    "async def run_agent_query(query: str, session_id: str):\n",
    "    \"\"\"Run a single query through the agent.\"\"\"\n",
    "    print(\"*****query*****\", query)\n",
    "    content = types.Content(parts=[types.Part(text=query)])\n",
    "    response_parts = []\n",
    "    sub_agents_used = set()\n",
    "    tools_used = []\n",
    "    \n",
    "    async for event in runner.run_async(\n",
    "        user_id=\"eval_user\",\n",
    "        session_id=session_id,\n",
    "        new_message=content\n",
    "    ):\n",
    "        # Collect response text and track tool calls from event.content.parts\n",
    "        if hasattr(event, 'content') and event.content:\n",
    "            for part in event.content.parts:\n",
    "                if hasattr(part, 'text') and part.text:\n",
    "                    response_parts.append(part.text)\n",
    "                # Tool calls in ADK are Part objects with function_call attribute\n",
    "                if hasattr(part, 'function_call') and part.function_call:\n",
    "                    fc = part.function_call\n",
    "                    name = getattr(fc, 'name', None) or getattr(fc, 'function_name', None)\n",
    "                    if name:\n",
    "                        tools_used.append(name)\n",
    "        \n",
    "        # Track sub-agents\n",
    "        if hasattr(event, 'author') and event.author:\n",
    "            if event.author != 'CruiseBookingAgent':\n",
    "                sub_agents_used.add(event.author)\n",
    "    \n",
    "    return {\n",
    "        'output': ' '.join(response_parts),\n",
    "        'sub_agents_used': list(sub_agents_used),\n",
    "        'tools_used': tools_used\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_with_example(example: Example) -> str:\n",
    "    # Create a session for all queries\n",
    "    session = asyncio.run(session_service.create_session(\n",
    "        app_name=\"cruise_booking_eval\",\n",
    "        user_id=\"eval_user\"\n",
    "    ))\n",
    "    result = asyncio.run(run_agent_query(example.input.get('Query'), session.id))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Expirement \n",
    "##### Run examples calling agent for each exaple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_run = False\n",
    "experiment = run_experiment(dataset,\n",
    "                            run_agent_with_example,\n",
    "                            dry_run=dry_run,\n",
    "                            evaluators=[],\n",
    "                            experiment_name=\"Cruise Booking Agent Eval\",\n",
    "                            experiment_description=\"Cruise Booking Agent Evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Code Based Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@create_evaluator(name=\"Tools Used Eval\", kind=\"CODE\")\n",
    "def evaluate_tool_calls(output: str) -> float:\n",
    "    try:\n",
    "        print(\"output\",output)\n",
    "        if output and output.get(\"tools_used\"):\n",
    "            if(len(output)>0):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "    except Exception as e:\n",
    "        print(\"Error in evaluate_tool_calls\",e)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_COMPLETENESS_TEMPLATE = all_prompts['response_completeness']['template']\n",
    "@create_evaluator(name=\"Response Completeness Eval\", kind=\"LLM\")\n",
    "def evaluate_response_completeness(input: dict, output: dict) -> bool:\n",
    "    if output is None:\n",
    "        return False\n",
    "    \n",
    "    query = input.get('question', '')\n",
    "    response = output.get('final_output', '')\n",
    "    \n",
    "    if not query or not response:\n",
    "        return False\n",
    "    \n",
    "    expected_info = \"cruise options with relevant details\"\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'input': [query],\n",
    "        'output': [response],\n",
    "        'expected_info': [expected_info]\n",
    "    })\n",
    "    \n",
    "    result = llm_classify(\n",
    "        data=df,\n",
    "        template=RESPONSE_COMPLETENESS_TEMPLATE,\n",
    "        rails=['1', '2', '3', '4', '5'],\n",
    "        model=OpenAIModel(model=\"openai/gpt-4.1-mini\"),\n",
    "        provide_explanation=True\n",
    "    )\n",
    "    \n",
    "    score = int(result['label'].iloc[0]) if 'label' in result.columns else 3\n",
    "    return score >= 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluators against Agent outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run code evaluators : evaluate_tool_calls\n",
    "- Run LLM-As-Judge Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = evaluate_experiment(experiment,\n",
    "                            evaluators=[evaluate_tool_calls,evaluate_response_completeness])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_notebooks1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}